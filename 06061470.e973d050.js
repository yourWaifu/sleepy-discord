(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{323:function(e,n,t){"use strict";t.d(n,"a",(function(){return l})),t.d(n,"b",(function(){return m}));var i=t(0),r=t.n(i);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}function c(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,i,r=function(e,n){if(null==e)return{};var t,i,r={},o=Object.keys(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)t=o[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var d=r.a.createContext({}),u=function(e){var n=r.a.useContext(d),t=n;return e&&(t="function"==typeof e?e(n):c(c({},n),e)),t},l=function(e){var n=u(e.components);return r.a.createElement(d.Provider,{value:n},e.children)},p={inlineCode:"code",wrapper:function(e){var n=e.children;return r.a.createElement(r.a.Fragment,{},n)}},b=r.a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,o=e.originalType,a=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),l=u(t),b=i,m=l["".concat(a,".").concat(b)]||l[b]||p[b]||o;return t?r.a.createElement(m,c(c({ref:n},d),{},{components:t})):r.a.createElement(m,c({ref:n},d))}));function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var o=t.length,a=new Array(o);a[0]=b;var c={};for(var s in n)hasOwnProperty.call(n,s)&&(c[s]=n[s]);c.originalType=e,c.mdxType="string"==typeof e?e:i,a[1]=c;for(var d=2;d<o;d++)a[d]=t[d];return r.a.createElement.apply(null,a)}return r.a.createElement.apply(null,t)}b.displayName="MDXCreateElement"},76:function(e,n,t){"use strict";t.r(n),t.d(n,"frontMatter",(function(){return a})),t.d(n,"metadata",(function(){return c})),t.d(n,"toc",(function(){return s})),t.d(n,"default",(function(){return u}));var i=t(3),r=t(7),o=(t(0),t(323)),a={title:"include/sleepy_discord/voice_connection.h"},c={unversionedId:"reference/Files/voice__connection_8h",id:"reference/Files/voice__connection_8h",isDocsHomePage:!1,title:"include/sleepy_discord/voice_connection.h",description:"Namespaces",source:"@site/docs/reference/Files/voice__connection_8h.md",slug:"/reference/Files/voice__connection_8h",permalink:"/sleepy-discord/docs/reference/Files/voice__connection_8h",version:"current",sidebar:"Reference",previous:{title:"sleepy_discord/voice_connection.cpp",permalink:"/sleepy-discord/docs/reference/Files/voice__connection_8cpp"},next:{title:"sleepy_discord/webhook.cpp",permalink:"/sleepy-discord/docs/reference/Files/webhook_8cpp"}},s=[{value:"Namespaces",id:"namespaces",children:[]},{value:"Classes",id:"classes",children:[]},{value:"Types",id:"types",children:[]},{value:"Types Documentation",id:"types-documentation",children:[{value:"enum AudioSourceType",id:"enum-audiosourcetype",children:[]},{value:"using AudioSample",id:"using-audiosample",children:[]},{value:"using AudioPointerSource",id:"using-audiopointersource",children:[]}]},{value:"Source code",id:"source-code",children:[]}],d={toc:s};function u(e){var n=e.components,t=Object(r.a)(e,["components"]);return Object(o.b)("wrapper",Object(i.a)({},d,t,{components:n,mdxType:"MDXLayout"}),Object(o.b)("h2",{id:"namespaces"},"Namespaces"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",{parentName:"tr",align:null},"Name"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Namespaces/namespace_sleepy_discord"},"SleepyDiscord")))))),Object(o.b)("h2",{id:"classes"},"Classes"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",{parentName:"tr",align:null}),Object(o.b)("th",{parentName:"tr",align:null},"Name"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"class"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/class_sleepy_discord_1_1_base_voice_event_handler"},"SleepyDiscord::BaseVoiceEventHandler")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_voice_context"},"SleepyDiscord::VoiceContext")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_audio_transmission_details"},"SleepyDiscord::AudioTransmissionDetails")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_base_audio_source"},"SleepyDiscord::BaseAudioSource")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_base_audio_output"},"SleepyDiscord::BaseAudioOutput")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_audio_timer"},"SleepyDiscord::AudioTimer")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"class"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/class_sleepy_discord_1_1_voice_connection"},"SleepyDiscord::VoiceConnection")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_basic_audio_source_for_containers"},"SleepyDiscord::BasicAudioSourceForContainers")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_audio_source"},"SleepyDiscord::AudioSource")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"struct"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Classes/struct_sleepy_discord_1_1_audio_vector_source"},"SleepyDiscord::AudioVectorSource")))))),Object(o.b)("h2",{id:"types"},"Types"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",{parentName:"tr",align:null}),Object(o.b)("th",{parentName:"tr",align:null},"Name"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"enum"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Files/voice__connection_8h#enum-audiosourcetype"},"AudioSourceType"))," { AUDIO_BASE_TYPE, AUDIO_CONTAINER}")),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"using int16_t"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Files/voice__connection_8h#using-audiosample"},"AudioSample")))),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"using BaseAudioSource"),Object(o.b)("td",{parentName:"tr",align:null},Object(o.b)("strong",{parentName:"td"},Object(o.b)("a",{parentName:"strong",href:"/docs/reference/Files/voice__connection_8h#using-audiopointersource"},"AudioPointerSource")))))),Object(o.b)("h2",{id:"types-documentation"},"Types Documentation"),Object(o.b)("h3",{id:"enum-audiosourcetype"},"enum AudioSourceType"),Object(o.b)("table",null,Object(o.b)("thead",{parentName:"table"},Object(o.b)("tr",{parentName:"thead"},Object(o.b)("th",{parentName:"tr",align:null},"Enumerator"),Object(o.b)("th",{parentName:"tr",align:null},"Value"),Object(o.b)("th",{parentName:"tr",align:null},"Description"))),Object(o.b)("tbody",{parentName:"table"},Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"AUDIO_BASE_TYPE"),Object(o.b)("td",{parentName:"tr",align:null}),Object(o.b)("td",{parentName:"tr",align:null})),Object(o.b)("tr",{parentName:"tbody"},Object(o.b)("td",{parentName:"tr",align:null},"AUDIO_CONTAINER"),Object(o.b)("td",{parentName:"tr",align:null}),Object(o.b)("td",{parentName:"tr",align:null})))),Object(o.b)("h3",{id:"using-audiosample"},"using AudioSample"),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-cpp"},"using SleepyDiscord::AudioSample = typedef int16_t;\n")),Object(o.b)("h3",{id:"using-audiopointersource"},"using AudioPointerSource"),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-cpp"},"using SleepyDiscord::AudioPointerSource = typedef BaseAudioSource;\n")),Object(o.b)("h2",{id:"source-code"},"Source code"),Object(o.b)("pre",null,Object(o.b)("code",{parentName:"pre",className:"language-cpp"},'#pragma once\n#include <vector>\n#include <array>\n#include <cstdint>\n#include <list>\n#if (!defined(NONEXISTENT_OPUS) && !defined(SLEEPY_DISCORD_CMAKE)) || defined(EXISTENT_OPUS)\n#include <opus.h>\n#endif\n#include "udp_client.h"\n#include "snowflake.h"\n#include "server.h"\n#include "channel.h"\n#include "message_receiver.h"\n#include "timer.h"\n\nnamespace SleepyDiscord {\n    using AudioSample = int16_t;\n\n    class BaseDiscordClient;\n    class VoiceConnection;\n\n    class BaseVoiceEventHandler {\n    public:\n        virtual ~BaseVoiceEventHandler() = default;\n        virtual void onReady(VoiceConnection&) {}\n        virtual void onSpeaking(VoiceConnection&) {}\n        virtual void onEndSpeaking(VoiceConnection&) {}\n        virtual void onFinishedSpeaking(VoiceConnection&) {}\n        virtual void onHeartbeat(VoiceConnection&) {}\n        virtual void onHeartbeatAck(VoiceConnection&) {}\n    };\n\n    struct VoiceContext {\n        friend VoiceConnection;\n        friend BaseDiscordClient;\n    public:\n        inline Snowflake<Channel> getChannelID() {\n            return channelID;\n        }\n\n        inline Snowflake<Server> getServerID() {\n            return serverID;\n        }\n\n        inline bool operator==(const VoiceContext& right) {\n            return this == &right;\n        }\n\n        inline void setVoiceHandler(BaseVoiceEventHandler* source) {\n            eventHandler = std::unique_ptr<BaseVoiceEventHandler>(source);\n        }\n\n        inline bool hasVoiceHandler() {\n            return eventHandler != nullptr;\n        }\n\n        inline BaseVoiceEventHandler& getVoiceHandler() {\n            return *(eventHandler.get());\n        }\n\n        template<class EventHandler, class... Types>\n        inline void startVoiceHandler(Types&&... arguments) {\n            setVoiceHandler(new EventHandler(std::forward<Types>(arguments)...));\n        }\n\n    private:\n        VoiceContext(Snowflake<Server> _serverID, Snowflake<Channel> _channelID, BaseVoiceEventHandler* _eventHandler) :\n            serverID(_serverID), channelID(_channelID), eventHandler(_eventHandler)\n        {}\n\n        Snowflake<Server> serverID;\n        Snowflake<Channel> channelID;\n        std::string sessionID = "";\n        std::string endpoint = "";\n        std::string token;\n        std::unique_ptr<BaseVoiceEventHandler> eventHandler;\n    };\n\n    enum AudioSourceType {\n        AUDIO_BASE_TYPE,\n        AUDIO_CONTAINER,\n    };\n\n    class VoiceConnection;\n\n    struct AudioTransmissionDetails {\n    public:\n        inline VoiceContext& context() {\n            return _context;\n        }\n\n        inline std::size_t amountSentSinceLastTime() {\n            return _amountSentSinceLastTime;\n        }\n\n        static inline constexpr int bitrate() {\n            return 48000;\n        }\n\n        static inline constexpr int channels() {\n            return 2;\n        }\n\n        static inline constexpr std::size_t proposedLengthOfTime() {\n            return 20;\n        }\n\n        static inline constexpr std::size_t proposedLength() {\n            return static_cast<std::size_t>(\n                bitrate() * channels() * (\n                    static_cast<float>(proposedLengthOfTime()) / 1000 /*millisecond conversion*/\n                )\n            );\n        }\n\n    private:\n        friend VoiceConnection;\n        AudioTransmissionDetails(\n            VoiceContext& con,\n            const std::size_t amo\n        ) :\n            _context(con),\n            _amountSentSinceLastTime(amo)\n        { }\n\n        VoiceContext& _context;\n        const std::size_t _amountSentSinceLastTime;\n    };\n\n    struct BaseAudioSource {\n        BaseAudioSource() : type(AUDIO_BASE_TYPE) {}\n        explicit BaseAudioSource(AudioSourceType typ) : type(typ) {}\n        virtual inline bool isOpusEncoded() { return false; }\n        const AudioSourceType type;\n        virtual ~BaseAudioSource() = default;\n        //This function below is here in case the user uses this class\n        virtual void read(AudioTransmissionDetails& /*details*/, int16_t*& /*buffer*/, std::size_t& /*length*/) {};\n\n        enum SpeakingFlag : unsigned int {\n            Microphone = 1u << 0u,\n            Soundshare = 1u << 1u,\n            Priority = 1u << 2u,\n        };\n        SpeakingFlag speakingFlag = Microphone;\n    };\n\n    struct BaseAudioOutput {\n        using Container = std::array<AudioSample, AudioTransmissionDetails::proposedLength()>;\n        BaseAudioOutput() = default;\n        virtual ~BaseAudioOutput() = default;\n        virtual void write(Container audio, AudioTransmissionDetails& details) {}\n    private:\n        friend VoiceConnection;\n    };\n\n    struct AudioTimer {\n        Timer timer;\n        time_t nextTime = 0;\n        void stop() {\n            if (timer.isValid())\n                timer.stop();\n        }\n    };\n\n    class VoiceConnection : public GenericMessageReceiver {\n    public:\n        VoiceConnection(BaseDiscordClient* client, VoiceContext& _context);\n        VoiceConnection(VoiceConnection&&) = default;\n\n        ~VoiceConnection() = default;\n\n        inline bool operator==(const VoiceConnection& right) {\n            return this == &right;\n        }\n\n        inline bool isReady() const {\n            return state & State::ABLE;\n        }\n\n        inline void setAudioSource(BaseAudioSource*& source) {\n            audioSource = std::unique_ptr<BaseAudioSource>(source);\n        }\n\n        inline bool hasAudioSource() const {\n            return audioSource != nullptr;\n        }\n\n        inline BaseAudioSource& getAudioSource() {\n            return *audioSource;\n        }\n\n        /*To do there might be a way to prevent code reuse here*/\n\n        inline void setAudioOutput(BaseAudioOutput*& output) {\n            audioOutput = std::unique_ptr<BaseAudioOutput>(output);\n        } \n\n        inline bool hasAudioOutput() const {\n            return audioOutput != nullptr;\n        }\n\n        inline BaseAudioOutput& getAudioOutput() {\n            return *audioOutput;\n        }\n\n        //=== startSpeaking functions ===\n\n        void startSpeaking();\n\n        inline void startSpeaking(BaseAudioSource* source) {\n            setAudioSource(source);\n            startSpeaking();\n        }\n\n        template<class AudioSource, class... Types>\n        inline void startSpeaking(Types&&... arguments) {\n            startSpeaking(new AudioSource(std::forward<Types>(arguments)...));\n        }\n\n        //=== startListening ===\n\n        void startListening();\n\n        inline BaseDiscordClient& getDiscordClient() {\n            return *origin;\n        }\n\n        inline BaseDiscordClient& getOrigin() {\n            return getDiscordClient();\n        }\n\n        inline VoiceContext& getContext() {\n            return context;\n        }\n\n        void speak(AudioSample*& audioData, const std::size_t& length);\n\n        void disconnect();\n\n        //Discord doens\'t gives the endpoint with wss:// or ?v=3, so it\'s done here\n        static std::string getWebSocketURI(const std::string& givenEndpoint) {\n            std::string endpoint;\n            //length of wss:///?v=3 is 11, plus one equals 12\n            endpoint.reserve(12 + givenEndpoint.length());\n            endpoint += "wss://";\n            endpoint += givenEndpoint;\n            endpoint += "/?v=3";\n            return endpoint;\n        }\n    private:\n        friend BaseDiscordClient;\n\n        void initialize() override;\n        void processMessage(const std::string &message) override;\n        void processCloseCode(const int16_t code) override;\n\n        enum VoiceOPCode {\n            IDENTIFY            = 0,  //client begin a voice websocket connection\n            SELECT_PROTOCOL     = 1,  //client select the voice protocol\n            READY               = 2,  //server complete the websocket handshake\n            HEARTBEAT           = 3,  //client keep the websocket connection alive\n            SESSION_DESCRIPTION = 4,  //server describe the session\n            SPEAKING            = 5,  //both   indicate which users are speaking\n            HEARTBEAT_ACK       = 6,  //server sent immediately following a received client heartbeat\n            RESUME              = 7,  //client resume a connection\n            HELLO               = 8,  //server the continuous interval in milliseconds after which the client should send a heartbeat\n            RESUMED             = 9,  //server acknowledge Resume\n            CLIENT_DISCONNECT   = 13  //server a client has disconnected from the voice channel\n        };\n            \n        enum State : uint8_t {\n            NOT_CONNECTED = 0 << 0,\n            CONNECTED     = 1 << 0,\n            OPEN          = 1 << 1,\n            AUDIO_ENABLED = 1 << 2,\n            SENDING_AUDIO = 1 << 3,\n\n            CAN_ENCODE    = 1 << 6,\n            CAN_DECODE    = 1 << 7,\n\n            ABLE          = CONNECTED | OPEN | AUDIO_ENABLED,\n        };\n\n#ifdef NONEXISTENT_OPUS\n        using OpusEncoder = void;\n        using OpusDecoder = void;\n#endif\n\n        BaseDiscordClient* origin;\n        VoiceContext& context;\n        UDPClient UDP;\n        time_t heartbeatInterval = 0;\n        uint32_t sSRC;\n        uint16_t port;\n        Timer heart;\n        State state = State::NOT_CONNECTED;\n        int16_t numOfPacketsSent = 0;\n        std::unique_ptr<BaseAudioSource> audioSource;\n        std::unique_ptr<BaseAudioOutput> audioOutput;\n        AudioTimer speechTimer;\n        AudioTimer listenTimer;\n        std::size_t samplesSentLastTime = 0;\n        time_t nextTime = 0;\n        OpusEncoder *encoder = nullptr;\n        OpusDecoder *decoder = nullptr;\n        uint16_t sequence = 0;\n        uint32_t timestamp = 0;\n\n        std::array<unsigned char, 32> secretKey;\n        static constexpr int nonceSize = 24;\n\n        //to do use this for events\n        template<class... Types>\n        inline void callEvent(void (BaseVoiceEventHandler::*member)(Types...), Types&&... arguments){\n            if(context.eventHandler != nullptr)\n                ((*context.eventHandler).*member)(arguments...);\n        }\n        void heartbeat();\n        inline void scheduleNextTime(AudioTimer& timer, TimedTask code, const time_t interval);\n        inline void stopSpeaking() {\n            state = static_cast<State>(state & ~SENDING_AUDIO);\n        }\n        void sendSpeaking(bool isNowSpeaking);\n        void speak();\n        void sendAudioData(\n            uint8_t*& encodedAudioData,\n            const std::size_t & length,\n            const std::size_t & frameSize\n        );\n        void listen();\n        void processIncomingAudio(const std::vector<uint8_t>& data);\n    };\n\n    struct BasicAudioSourceForContainers : public BaseAudioSource {\n        BasicAudioSourceForContainers() : BaseAudioSource(AUDIO_CONTAINER) {}\n        virtual void speak(\n            VoiceConnection& connection,\n            AudioTransmissionDetails& details,\n            std::size_t& length\n        ) = 0;\n    };\n\n    template<class _Container>\n    struct AudioSource : public BasicAudioSourceForContainers {\n    public:\n        using Container = _Container;\n        AudioSource() : BasicAudioSourceForContainers() {}\n        virtual void read(AudioTransmissionDetails& /*details*/, int16_t*& /*buffer*/, std::size_t& /*length*/)  override {};\n        virtual void read(AudioTransmissionDetails& details, Container& target) {};\n    private:\n        friend VoiceConnection;\n        void speak(\n            VoiceConnection& connection,\n            AudioTransmissionDetails& details,\n            std::size_t& length\n        ) override {\n            read(details, containedAudioData);\n            int16_t* audioBuffer = containedAudioData.data();\n            length = containedAudioData.size();\n            connection.speak(audioBuffer, length);\n        }\n    protected:\n        Container containedAudioData;\n    };\n\n    struct AudioVectorSource : public AudioSource<std::vector<AudioSample>> {\n    public:\n        AudioVectorSource() : AudioSource<std::vector<AudioSample>>() {\n            containedAudioData.resize(AudioTransmissionDetails::proposedLength());\n        }\n    };\n\n    using AudioPointerSource = BaseAudioSource;\n}\n')),Object(o.b)("hr",null),Object(o.b)("p",null,"Updated on 27 July 2021 at 06:05:20 UTC"))}u.isMDXComponent=!0}}]);